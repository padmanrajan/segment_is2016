\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2016}

\usepackage{graphicx}
\usepackage{amssymb,amsmath,bm}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage{cite}
\usepackage{upgreek}

\def\vec#1{\ensuremath{\bm{{#1}}}}
\def\mat#1{\vec{#1}}


\sloppy % better line breaks
\ninept

\title{Entropy-based segmentation of birdcalls using Fourier transform phase}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If multiple authors, uncomment and edit the lines shown below.       %%
%% Note that each line must be emphasized {\em } by itself.             %%
%% (by Stephen Martucci, author of spconf.sty).                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\makeatletter
%\def\name#1{\gdef\@name{#1\\}}
%\makeatother
%\name{{\em Firstname1 Lastname1, Firstname2 Lastname2, Firstname3 Lastname3,}\\
%      {\em Firstname4 Lastname4, Firstname5 Lastname5, Firstname6 Lastname6,
%      Firstname7 Lastname7}}
%%%%%%%%%%%%%%% End of required multiple authors changes %%%%%%%%%%%%%%%%%

\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother \name{{\em Author Name$^1$, Co-author Name$^2$}}

\address{$^1$Author Affiliation \\
  $^2$Co-author Affiliation \\
  {\small \tt author@university.edu, coauthor@company.com}
}

%\twoauthors{Karen Sp\"{a}rck Jones.}{Department of Speech and Hearing \\
%  Brittania University, Ambridge, Voiceland \\
%  {\small \tt Karen@sh.brittania.edu} }
%  {Rose Tyler}{Department of Linguistics \\
%  University of Speechcity, Speechland \\
%  {\small \tt RTyler@ling.speech.edu} }

%
\begin{document}

  \maketitle
  %
  \begin{abstract}
  In this  paper we describe an entropy-based algorithm for the segmentation of
  birdcalls from recordings. The entropy of time-frequency blocks are estimated
  from the phase of the Fourier transform. To overcome difficulties in
  processing the phase, the group delay function from an all-pole filter is
  utilised. The group delay function has good frequency resolution properties,
  and hence provides relaiable estimates of the entropy. Furthermore, spectral
  whitening is performed to smooth the entropy estimate and the extremities are
  determined. A threshold is applied on the difference to distinguish the call
  periods from the background.  The algorithm is evaluated on two different
  datasets, one of which is recorded in more challenging field conditions. When
  compared to entropy estimated from the power spectrum, the entropy from the
  group delay function provides better detection accuracy at almost all
  operating points. The choice of model order of the all-pole filter for
  different bird species is also briefly investigated.
  \end{abstract}
  \noindent{\bf Index Terms}: bioacoustics, birdcall segmentation, Fourier transform phase
  



\section{Introduction}

With the advent of automated recording devices, the collection of large amounts of
bioacoustic data has become relatively easy. By analysing birdcalls collected in
this manner, it is possible to perform tasks such as the tracking of migrant
species or examining the avian biodiversity of a given region. Typically, the
collected data is processed offline. In this process, the first step is usually to
determine regions of interest in the recording. An entropy-based bird phrase segmentation
technique was developed in \cite{wang2013}. In this paper, we propose a modified
version of that technique, by using information from the phase of the short-term
Fourier transform (STFT.) Most techniques for processing speech and audio signals have
utilised the magnitude spectrum of the STFT. Although the phase spectrum of the STFT has 
useful information, its processing has remained difficult. A popular technique
for exploiting information from the phase has been through group delay functions. In
this work, we utilise information from group delay functions using parametric
models, and apply it to segment birdcalls into active and inactive regions.

The group delay function has good frequency resolution propreties, which enable
it to be useful in tasks such as speech recognition and speaker recognition
\cite{hema} \cite{padman} \cite{modgdf}. The same property is beneficial in the processing of bird
vocalizations. In \cite{wang2013}, the entropy within a sliding time-frequency
window over the spectrogram has been effectively used for distinguishing active
and inactive regions. The essential idea is that birdcalls have more structure
(for eg.~harmonics may be present), and thus have lower entropy when compared to
 background sounds, which have higher entropy. This difference in entropy
levels enable effective distinction between birdcalls and the background.
%Applying a similar technique with the group delay function, rather than the
%spectrogram, provides increased frequency resolution, and hence more effective
%entropy computation. 
The high resolution property of group delay functions enable accurate tracking
of time-frequency information \cite{padman}. In this work, the entropy of a sliding
time-frequency window is estimated from the group delay representation. Spectral
whitening is applied to smooth the entropy estimates and thresholding on differences of extrema is
applied to separate the birdcalls from the background.

Most of the bioacoustic studies have used manually segmented bird calls \cite{Trifa} \cite{Lee} \cite{Kaewtip}. Time domain segmentation using energy has been used in many studies \cite{Harma} \cite{Somervuo} \cite{Fagerlund} . The energy  based segmentation method is highly influenced by background noise and will  where bird calls have low energy in comparison to the background. A KL-divergence based segmentation method   is proposed in \cite{Lakshmi}.  KL-divergence between normalized power spectral density of a frame and uniform distribution is computed. Local minima of KL divergence act as change points for bird vocalizations. In \cite{Neal} , time-frequency domain based segmentation using random forest classifier is proposed to segment the syllables from noisy audio signal. 





\section{Utilising Fourier transform phase}

Commonly used features for processing speech and audio signals are based on the
magnitude spectrum of the short-term Fourier transform. The phase spectrum has
received relatively lesser attention due to signal processing difficulties, one
of them being the need to unwrap the phase spectrum. The unwrapping problem can be
bypassed by utilising the group delay function, which is the negative derivative
of the phase spectrum. The group delay function can be
computed using properties of the Fourier transform, and hence avoids the need
for explicit computation of the phase spectrum \cite{gdDeriv}. 
However, this method can produce
artifacts in the form of spurious peaks at spectral nulls. These nulls
correspond to zeros close to the unit circle when the vocal tract transfer
function is represented in the $Z$ domain. 
%To avoid these spurious peaks, a
%minimum-phase assumption is made on the 
%underlying system. 
Several methods have been proposed in the literature to overcome the effects due
to these artifacts \cite{modgdf, productSpectrum}. Another technique to overcome
this difficulty is to model the vocal tract as an all-pole filter, hence
avoiding the nulls altogether. Such a
technique derived using linear prediction analysis was used in the detection of
formants in human speech \cite{yegnaFormant}. 


There is strong evidence that birds use their vocal tract as a selective filter
to modify the final sound \cite{catchpole}.  Given this, the source-filter model
developed for analysing human speech can be applied to bird vocalizations as
well.  Linear predictive (LP) analysis of human speech signals models the vocal
tract spectrum as an all-pole filter \cite{makhoul} excited by a single source.
When applied to birdcalls, this is a simplification of the `two-voice' theory of
avian vocalization\cite{catchpole}, in that there is assumed to be only one
source, rather than two. Nevertheless, this reasonable assumption is followed in
this work. A similar assumption has been made in \cite{agnihotri}, where LP
analysis has been applied in analysing the song of the greater racket-tailed
drongo.

The vocal tract is represented in the LP model as
\begin{equation}
H(\omega) = \frac{G}{1-\sum_{k=1}^{P} a(k) e^{-j \omega k}},
\end{equation}
where the predictor model order is $P$, $G$ represents the gain and $a(k)$ are
the predictor coefficientses\cite{makhoul}.
The filter reprsented by $H(\omega)$ is an all-pole filter, and its group delay
function does not suffer from the artifacts mentioned earlier. The group delay
function computed in this manner is termed as all-pole group delay
function (APGDF.) Figure \ref{fig:all-pole} shows the magnitude spectrum, LP
spectrum and APGDF derived from a 20 ms call of Cassins vireo (\textit{Vireo
cassinii.}) As can be seen, the APGDF emphasises the formants, as compared to the
DFT magnitude spectrum or the LP magnitude spectrum. Two peaks which are merged
in the magnitude spectra around the 100th frequency bin appear distinctly in the
APGDF.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth,height=7cm]
{apGd.eps}
\caption{based on all pole model (in the bottom panel).}
\label{fig:all-pole}
\end{figure}


Recently, feature vectors
derived from such a representation were used in speech \cite{drugman} and speaker
recognition \cite{padman}. 



%This is equivalent to a cascade of several
%first-order and second-order all-pole filters. The overall magnitude spectrum is
%the product of the individual magnitude spectra, and the overall phase spectrum
%is the sum of the individual phase spectra \cite{yegnaFormant}. Although
%developed for human speech, th






\section{Entropy-based segmentation of birdsong}

Computing the APGDF for every frame enables good time-frequency resolution of audio recording. Unlike the power spectrum, the APGDF can be negative. 
Since here we are interested only in the magnitudes and
locations of the frequency components, the sign of the APGDF is
ignored by taking the absolute value. Henceforth, APGDF means positive APGDF. 
 A sliding time-frequency window of width $w$ frames and frequency range
$fmin$ to $fmax$ is considered over the APGDF vectors. As in \cite{wang2013},
the entropy of this window is estimated using the expression
\begin{equation}
h_{k}=-\sum_{n=kT+1}^{kT+w}\sum_{f=fmin}^{fmax} \uptau_N(n,f) \ln \uptau_N(n,f),
\end{equation}
where $T$ is time-frequency window shift and $\uptau_N(n,f)$ is normalized APGDF phase spectrum.


$\uptau_N(n,f)$ is estimated using the expression \cite{wang2013}
\begin{equation}
\uptau_N(n,f)=\frac {\uptau(n,f)}
{\sum_{n=kT+1}^{kT+w}\sum_{f=fmin}^{fmax} \uptau(n,f)},
\end{equation}

where $\uptau(n,f)$ is APGDF phase spectrum at time $n$ and frequency  \textit{f}.

The entropy of the time-frequency windows containing bird call is lower than the one containing the background. The entropy calculated from block is less susceptible to the sudden changes in background as compared to the entropy calculated at each time instance \cite{wang2013}.

%The APGDF from figure
%\ref{fig:apgd} is shown in \ref{fig:apgdNoSign} after ignoring the sign.
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.5\textwidth,height=2cm]{apgdNoSign.eps}
%\caption{based on all pole model (in the bottom panel).}
%\label{fig:all-pole}
%\end{figure}



 


\subsection{Whitening APGDF before Entropy Calculation}

The drop in entropy at the presence of bird vocalizations makes it possible to detect call periods. This drop can only be detected accurately if background entropy is not changing drastically.  However this is not always the case in raw sound recordings. Due to the presence of various background sounds e.g. rain, thunder, other animals etc., entropy of the background can  vary rapidly making it difficult to separate it from entropy  at a call period \cite{wang2013}. To overcome this problem, whole APGDF spectrum is whitened  before calculating entropy. The entropy calculated from whitened APGDF spectrum is almost constant for background but dips enough to mark the presence of bird vocalizations. Due to this nature of background entropy, even a small change in entropy can be detected easily.

To whiten the APGDF spectrum ($\uptau(n,f)$), the covariance matrix of mean subtracted APGDF spectrum is calculated. The eigen values matrix (\textit{S}) and eigen vectors matrix (\textit{U}) of this covariance matrix are calculated. The white APGDF spectrum matrix ($\uptau_W(n,f)$) is calculated using the following expression:

\begin{equation}
\uptau_W(n,f)=diag \bigg(  \frac{1}{\sqrt{diag(S)+\epsilon}} \bigg )*U'*\uptau(n,f),
\end{equation}




The difference between entropy calculated from normal phase spectrum and whitened phase spectrum is evident in Figure \ref{fig:entropy}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth,height=7.5cm]{Entropy_gd_white_non_white2.eps}
\caption{Entropy calculated from phase spectrum and whitened phase spectrum respecively.}
\label{fig:entropy}
\end{figure}



\subsection{Detecting change points using thresholding}



   
To detect the change points, extrema-based thresholding is used. Local minima and maxima are estimated on the entropy.  The thresholding is applied on the difference of  consecutive local maximum and local minimum to find out the change point.  Two contiguous change points correspond to the start and end of a bird vocalization. These change points can be tracked back to get the start and end time of the vocalization in sound recording.     
   
   
   




\section{Experimentation and Performance Analysis}
The propose algorithm is evaluated on two datasets: Cassin’s Vireo \cite{data1} and MLSP Bird Classification Challaenge 2013 dataset \cite{data2}. In Cassin's Vireo dataset, the total duration of recordings is about 45 minutes. Out of 45 minutes, about 5 minutes of recordings correspond to the phrases of Cassin's Vireo. To calculate APGDF spectrum, frame length of 20 ms and increment of  
5 ms is used. The time-frequency window of 138.8 ms is used along with increment of 15 ms to calculate entropy. The frequency range of the block is from 1.5 kHz to 7 kHz \cite{wang2013}.Phrase annotations are available with the dataset \cite{data1}. These annotations are used as ground truth for evaluation of the proposed approach. For performance analysis, receiver operating characteristic (ROC) curves are used. A ROC curve is a  plot between false alarm rate (FAR) and true positive rate (TPR). 
True positive rate and false alarm rate are calculated using following equations:

\begin{equation}
TPR (\%)=\frac{\text{Correctly Classified Call Frames}} {\text{Total Call Frames}} \times 100 
\end{equation}




\begin{equation}
FAR (\%)=\frac{\text{Wrongly Classified As Call Frames}} {\text{Total Background Frames}} \times 100 
\end{equation}



 Figure \ref{fig:ROCdata1} depicts ROC curves comparing performance of methods based on entropy calculated from whitened spectrogram and entropy calculated from whitened group delay phase spectrum. It is clear from ROC plot in Figure \ref{fig:ROCdata1} that whitened APGDF method is outperforming the technique in \cite{wang2013} using whitened spectrogram and thresholding.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth,height=7cm]{gd_1.eps}
\caption{ROC  curves  comparing  performance  of  methods based on white phase spectrum and white power spectrum on Cassin's Vireo dataset.}
\label{fig:ROCdata1}
\end{figure}



 
 
 
The second dataset used is a subset of MLSP bird classification challenge 2013 dataset \cite{data2}. The data is recorded in more challenging conditions and recordings have low signal to noise ratio. Twenty Five files of ten seconds length are used to evaluate the proposed method.  Figure \ref{fig:ROCdata2} shows ROC curves comparing performance of methods based on entropy calculated from whitened APDGF spectrum and whitened power spectrum. Here group delay phase spectrum based method is outperforming spectrogram based method for almost all the operating points. 

 
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth,height=7cm] {gd_2.eps}
	\caption{ROC curves comparing performance of methods based on white phase spectrum and white power spectrum on MLSP 2013 single species dataset }   
	\label{fig:ROCdata2}
\end{figure} 




 \subsection{Model Order  vs AIC}
 
 
 To establish the optimum model order, Akaike Information criteria (AIC)  \cite{makhoul} is used. Figure \ref{fig:AIC_cassins} depicts the AIC for different model orders for Cassin's Vireo phrases.  
 
 
 \begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth,height=6cm] {cassins_AIC.eps}
	\caption{AIC vs model order for Cassin's Vireo }   
	\label{fig:AIC_cassins}
\end{figure} 

 
 
Figure \ref{fig:AIC_3} depicts the AIC for different model orders for three different bird species i.e. Cuckoo, Great Barbet and Laughing Thresh. 

 \begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth,height=7 cm] {model_order_vs_AIC.eps}
	\caption{AIC vs model order for three different species }   
	\label{fig:AIC_3}
\end{figure} 


 
 
 



%%  write formullae for calculating correct, missed detection and false alarms
\begin{comment}
Table 1 shows results generated by applying entropy based segmentation with thresholding on different sound recordings. 

  



\begin{table}[]
	\centering
	\caption{Table showing Correct (\%) Missed Detection(\%) and False alarm (\%) for particular thresholds and moving average windows } 
	\label{Table 1}

	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{File} & \textbf{Span} & \textbf{Threshold} & \textbf{Correct (\%)} & \textbf{False Alarm (\%)} & \textbf{Missed Detection (\%)} \\ \hline
		1.wav         & 9             & 1.2                & 77.7                  & 15.6                      & 60.5                           \\
	\hline	2.wav         & 3             & 1.45               & 73                    & 45                        & 25                             \\
		\hline	3.wav         & 3             & 1.5                & 71                    & 22                        & 54                             \\
		\hline	4.wav         & 9             & 1.15               & 75.1              & 21                  & 51.5                       \\
		\hline	5.wav         & 7             & 1.5                & 76.5                  & 18.5                      & 45                             \\
	\hline		6.wav         & 9             & 1.25               & 82.1              & 15.07                  & 32.2                       \\
		\hline	7.wav         & 7             & 1.25               & 79.6              & 17.8                  & 46.7                       \\
		\hline	8.wav         & 9             & 1.15               & 78.8              & 19.3                  & 35.1                      \\
		\hline	9.wav         & 5             & 1.5                & 86.06              & 13.6                  & 17.9                       \\
	\hline		10.wav        & 9             & 1.6                & 79.6              & 18.3                  & 28.31                       \\
		\hline	11.wav        & 3             & 1.45               & 91.1                  & 32.7                      & 7.53                           \\
		\hline	12.wav        & 7             & 1.4                & 85.1              & 11.3                   & 38.1      \\ \hline               	
	 
	\end{tabular}

\end{table}

\end{comment}
 
 
\section{Conclusion}
We propose an entropy based bird vocalization segmentation method where entropy is calculated from Group Delay phase spectrum. It is also established that whitening the power or phase spectrum before entropy calculation improves the performance of entropy based segmentation. From experimentation,it is clear that the proposed group delay based method outperforms the power spectrum based method.  



  \newpage
  \eightpt
  \bibliographystyle{IEEEtran}

  \bibliography{mybib}

%  \begin{thebibliography}{9}
%    \bibitem[1]{Davis80-COP}
%      S.\ B.\ Davis and P.\ Mermelstein,
%      ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%      \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
%    \bibitem[2]{Rabiner89-ATO}
%      L.\ R.\ Rabiner,
%      ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%      \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
%    \bibitem[3]{Hastie09-TEO}
%      T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%      \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%      New York: Springer, 2009.
%    \bibitem[4]{YourName16-XXX}
%      F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%      ``Title of your INTERSPEECH 2016 publication,''
%      in \textit{Interspeech 2016 -- 16\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 8–12, San Francisco, California, USA, Proceedings, Proceedings}, 2016, pp.~100--104.
%  \end{thebibliography}

\end{document}
